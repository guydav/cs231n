{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guydavidson/anaconda/envs/fast-ai/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.363906\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** As the weights are random and uniform, we expect the class probabilities to be random and uniform as well. As we normalize, we expect each class probability to be about 1/k, for k classes. That includes the correct class probability (y_i), and hence the loss function for each example will be (on expectation) -log(1/k) = -log(0.1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.021097 analytic: -1.021097, relative error: 2.337798e-08\n",
      "numerical: -0.198407 analytic: -0.198407, relative error: 6.628186e-08\n",
      "numerical: 0.843434 analytic: 0.843434, relative error: 6.708236e-08\n",
      "numerical: -2.463533 analytic: -2.463533, relative error: 4.442393e-08\n",
      "numerical: 2.231680 analytic: 2.231679, relative error: 1.714866e-08\n",
      "numerical: -0.715955 analytic: -0.715955, relative error: 1.054167e-07\n",
      "numerical: -1.801875 analytic: -1.801875, relative error: 1.668338e-08\n",
      "numerical: 0.135292 analytic: 0.135292, relative error: 1.839153e-07\n",
      "numerical: -2.791508 analytic: -2.791508, relative error: 3.963117e-09\n",
      "numerical: -2.655600 analytic: -2.655600, relative error: 1.131954e-08\n",
      "numerical: -0.467616 analytic: -0.467616, relative error: 1.562108e-07\n",
      "numerical: -0.338037 analytic: -0.338037, relative error: 6.387289e-08\n",
      "numerical: 0.507842 analytic: 0.507842, relative error: 7.768526e-08\n",
      "numerical: 0.345989 analytic: 0.345989, relative error: 1.995682e-08\n",
      "numerical: -1.144664 analytic: -1.144664, relative error: 1.505588e-08\n",
      "numerical: 0.531582 analytic: 0.531582, relative error: 1.152711e-07\n",
      "numerical: -0.857317 analytic: -0.857317, relative error: 2.148675e-08\n",
      "numerical: -0.269484 analytic: -0.269484, relative error: 5.977216e-08\n",
      "numerical: 0.666529 analytic: 0.666529, relative error: 3.883347e-08\n",
      "numerical: -0.880380 analytic: -0.880380, relative error: 3.240693e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.363906e+00 computed in 0.177444s\n",
      "vectorized loss: 2.363906e+00 computed in 0.002689s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 774.688184\n",
      "iteration 100 / 1000: loss 284.335092\n",
      "iteration 200 / 1000: loss 105.261172\n",
      "iteration 300 / 1000: loss 39.984193\n",
      "iteration 400 / 1000: loss 15.963662\n",
      "Stopping on iteration 435 since loss stagnates (loss = 11.853622, delta = 0.003766)\n",
      "1e-07 25000.0 100 0.291142857143 0.311\n",
      "Found best validation accuracy so far 0.311\n",
      "iteration 0 / 1000: loss 767.866448\n",
      "iteration 100 / 1000: loss 282.125719\n",
      "iteration 200 / 1000: loss 104.502826\n",
      "iteration 300 / 1000: loss 39.676073\n",
      "iteration 400 / 1000: loss 15.802961\n",
      "Stopping on iteration 486 since loss stagnates (loss = 7.896495, delta = 0.000742)\n",
      "1e-07 25000.0 200 0.304734693878 0.308\n",
      "iteration 0 / 1000: loss 782.463991\n",
      "iteration 100 / 1000: loss 287.391487\n",
      "iteration 200 / 1000: loss 106.482859\n",
      "iteration 300 / 1000: loss 40.294545\n",
      "iteration 400 / 1000: loss 16.092392\n",
      "Stopping on iteration 451 since loss stagnates (loss = 10.492928, delta = 0.005493)\n",
      "1e-07 25000.0 500 0.298775510204 0.327\n",
      "Found best validation accuracy so far 0.327\n",
      "iteration 0 / 1000: loss 787.878848\n",
      "iteration 100 / 1000: loss 289.034196\n",
      "iteration 200 / 1000: loss 107.086810\n",
      "iteration 300 / 1000: loss 40.495964\n",
      "iteration 400 / 1000: loss 16.129262\n",
      "Stopping on iteration 477 since loss stagnates (loss = 8.597561, delta = 0.005670)\n",
      "1e-07 25000.0 1000 0.307367346939 0.334\n",
      "Found best validation accuracy so far 0.334\n",
      "iteration 0 / 1000: loss 780.963754\n",
      "iteration 100 / 1000: loss 287.041127\n",
      "iteration 200 / 1000: loss 106.362583\n",
      "iteration 300 / 1000: loss 40.304738\n",
      "iteration 400 / 1000: loss 16.051799\n",
      "iteration 500 / 1000: loss 7.215011\n",
      "Stopping on iteration 536 since loss stagnates (loss = 5.658847, delta = 0.009271)\n",
      "1e-07 25000.0 1500 0.312306122449 0.318\n",
      "iteration 0 / 1000: loss 953.639077\n",
      "iteration 100 / 1000: loss 272.471437\n",
      "iteration 200 / 1000: loss 79.083506\n",
      "iteration 300 / 1000: loss 23.979521\n",
      "Stopping on iteration 357 since loss stagnates (loss = 12.807747, delta = 0.004636)\n",
      "1e-07 31250.0 100 0.285102040816 0.288\n",
      "iteration 0 / 1000: loss 968.689000\n",
      "iteration 100 / 1000: loss 276.255529\n",
      "iteration 200 / 1000: loss 80.082026\n",
      "iteration 300 / 1000: loss 24.237760\n",
      "Stopping on iteration 382 since loss stagnates (loss = 10.101580, delta = 0.006719)\n",
      "1e-07 31250.0 200 0.297244897959 0.294\n",
      "iteration 0 / 1000: loss 972.769084\n",
      "iteration 100 / 1000: loss 277.838978\n",
      "iteration 200 / 1000: loss 80.538656\n",
      "iteration 300 / 1000: loss 24.400350\n",
      "iteration 400 / 1000: loss 8.459589\n",
      "Stopping on iteration 443 since loss stagnates (loss = 5.820938, delta = 0.001006)\n",
      "1e-07 31250.0 500 0.308346938776 0.316\n",
      "iteration 0 / 1000: loss 971.865733\n",
      "iteration 100 / 1000: loss 277.576285\n",
      "iteration 200 / 1000: loss 80.460003\n",
      "iteration 300 / 1000: loss 24.384194\n",
      "iteration 400 / 1000: loss 8.465292\n",
      "Stopping on iteration 459 since loss stagnates (loss = 5.139474, delta = 0.000079)\n",
      "1e-07 31250.0 1000 0.311755102041 0.331\n",
      "iteration 0 / 1000: loss 969.609092\n",
      "iteration 100 / 1000: loss 276.860745\n",
      "iteration 200 / 1000: loss 80.282130\n",
      "iteration 300 / 1000: loss 24.342377\n",
      "iteration 400 / 1000: loss 8.428920\n",
      "Stopping on iteration 481 since loss stagnates (loss = 4.406573, delta = 0.001934)\n",
      "1e-07 31250.0 1500 0.315224489796 0.322\n",
      "iteration 0 / 1000: loss 1144.746376\n",
      "iteration 100 / 1000: loss 254.489324\n",
      "iteration 200 / 1000: loss 57.864052\n",
      "iteration 300 / 1000: loss 14.449465\n",
      "Stopping on iteration 308 since loss stagnates (loss = 13.116190, delta = 0.005302)\n",
      "1e-07 37500.0 100 0.282326530612 0.296\n",
      "iteration 0 / 1000: loss 1150.082202\n",
      "iteration 100 / 1000: loss 255.681072\n",
      "iteration 200 / 1000: loss 58.034328\n",
      "iteration 300 / 1000: loss 14.478431\n",
      "Stopping on iteration 361 since loss stagnates (loss = 7.124929, delta = 0.004737)\n",
      "1e-07 37500.0 200 0.295408163265 0.321\n",
      "iteration 0 / 1000: loss 1167.655232\n",
      "iteration 100 / 1000: loss 259.551787\n",
      "iteration 200 / 1000: loss 59.015801\n",
      "iteration 300 / 1000: loss 14.710697\n",
      "Stopping on iteration 368 since loss stagnates (loss = 6.648392, delta = 0.006097)\n",
      "1e-07 37500.0 500 0.299367346939 0.31\n",
      "iteration 0 / 1000: loss 1158.762419\n",
      "iteration 100 / 1000: loss 257.662259\n",
      "iteration 200 / 1000: loss 58.654276\n",
      "iteration 300 / 1000: loss 14.643275\n",
      "Stopping on iteration 390 since loss stagnates (loss = 5.359704, delta = 0.003353)\n",
      "1e-07 37500.0 1000 0.302346938776 0.312\n",
      "iteration 0 / 1000: loss 1151.643800\n",
      "iteration 100 / 1000: loss 256.071546\n",
      "iteration 200 / 1000: loss 58.293860\n",
      "iteration 300 / 1000: loss 14.556470\n",
      "iteration 400 / 1000: loss 4.882504\n",
      "Stopping on iteration 415 since loss stagnates (loss = 4.330205, delta = 0.007187)\n",
      "1e-07 37500.0 1500 0.308204081633 0.319\n",
      "iteration 0 / 1000: loss 1345.083679\n",
      "iteration 100 / 1000: loss 232.723676\n",
      "iteration 200 / 1000: loss 41.788242\n",
      "Stopping on iteration 288 since loss stagnates (loss = 10.652451, delta = 0.001407)\n",
      "1e-07 43750.0 100 0.289367346939 0.296\n",
      "iteration 0 / 1000: loss 1352.055550\n",
      "iteration 100 / 1000: loss 234.102519\n",
      "iteration 200 / 1000: loss 42.054387\n",
      "iteration 300 / 1000: loss 8.983873\n",
      "Stopping on iteration 331 since loss stagnates (loss = 6.166725, delta = 0.001492)\n",
      "1e-07 43750.0 200 0.301346938776 0.316\n",
      "iteration 0 / 1000: loss 1365.563465\n",
      "iteration 100 / 1000: loss 236.372455\n",
      "iteration 200 / 1000: loss 42.375622\n",
      "iteration 300 / 1000: loss 9.044080\n",
      "Stopping on iteration 353 since loss stagnates (loss = 4.880139, delta = 0.007674)\n",
      "1e-07 43750.0 500 0.301897959184 0.318\n",
      "iteration 0 / 1000: loss 1361.958806\n",
      "iteration 100 / 1000: loss 235.646090\n",
      "iteration 200 / 1000: loss 42.273325\n",
      "iteration 300 / 1000: loss 9.028417\n",
      "Stopping on iteration 367 since loss stagnates (loss = 4.252668, delta = 0.009459)\n",
      "1e-07 43750.0 1000 0.304428571429 0.323\n",
      "iteration 0 / 1000: loss 1336.936444\n",
      "iteration 100 / 1000: loss 231.334287\n",
      "iteration 200 / 1000: loss 41.513721\n",
      "iteration 300 / 1000: loss 8.895001\n",
      "Stopping on iteration 364 since loss stagnates (loss = 4.352765, delta = 0.001964)\n",
      "1e-07 43750.0 1500 0.301795918367 0.315\n",
      "iteration 0 / 1000: loss 1548.732864\n",
      "iteration 100 / 1000: loss 208.760297\n",
      "iteration 200 / 1000: loss 29.737281\n",
      "Stopping on iteration 293 since loss stagnates (loss = 6.437527, delta = 0.008520)\n",
      "1e-07 50000.0 100 0.29593877551 0.305\n",
      "iteration 0 / 1000: loss 1554.997838\n",
      "iteration 100 / 1000: loss 209.420350\n",
      "iteration 200 / 1000: loss 29.791909\n",
      "iteration 300 / 1000: loss 5.808844\n",
      "Stopping on iteration 323 since loss stagnates (loss = 4.492067, delta = 0.000141)\n",
      "1e-07 50000.0 200 0.295673469388 0.301\n",
      "iteration 0 / 1000: loss 1557.585152\n",
      "iteration 100 / 1000: loss 209.810698\n",
      "iteration 200 / 1000: loss 29.862239\n",
      "iteration 300 / 1000: loss 5.863410\n",
      "Stopping on iteration 333 since loss stagnates (loss = 4.073396, delta = 0.008073)\n",
      "1e-07 50000.0 500 0.301693877551 0.322\n",
      "iteration 0 / 1000: loss 1538.727762\n",
      "iteration 100 / 1000: loss 207.307001\n",
      "iteration 200 / 1000: loss 29.537115\n",
      "iteration 300 / 1000: loss 5.805834\n",
      "Stopping on iteration 350 since loss stagnates (loss = 3.502009, delta = 0.001717)\n",
      "1e-07 50000.0 1000 0.300836734694 0.314\n",
      "iteration 0 / 1000: loss 1507.550122\n",
      "iteration 100 / 1000: loss 202.934255\n",
      "iteration 200 / 1000: loss 28.938983\n",
      "iteration 300 / 1000: loss 5.729989\n",
      "Stopping on iteration 377 since loss stagnates (loss = 2.906323, delta = 0.009975)\n",
      "1e-07 50000.0 1500 0.304734693878 0.324\n",
      "iteration 0 / 1000: loss 787.416038\n",
      "Stopping on iteration 28 since loss stagnates (loss = 2.629723, delta = 0.008376)\n",
      "2.575e-06 25000.0 100 0.240673469388 0.243\n",
      "iteration 0 / 1000: loss 765.820341\n",
      "Stopping on iteration 28 since loss stagnates (loss = 2.571263, delta = 0.001596)\n",
      "2.575e-06 25000.0 200 0.293183673469 0.292\n",
      "iteration 0 / 1000: loss 781.501252\n",
      "Stopping on iteration 42 since loss stagnates (loss = 2.127295, delta = 0.000377)\n",
      "2.575e-06 25000.0 500 0.325346938776 0.331\n",
      "iteration 0 / 1000: loss 774.721542\n",
      "Stopping on iteration 35 since loss stagnates (loss = 2.127055, delta = 0.005060)\n",
      "2.575e-06 25000.0 1000 0.31706122449 0.342\n",
      "Found best validation accuracy so far 0.342\n",
      "iteration 0 / 1000: loss 765.637985\n",
      "Stopping on iteration 34 since loss stagnates (loss = 2.170440, delta = 0.004303)\n",
      "2.575e-06 25000.0 1500 0.324224489796 0.337\n",
      "iteration 0 / 1000: loss 961.293958\n",
      "Stopping on iteration 33 since loss stagnates (loss = 2.165171, delta = 0.003826)\n",
      "2.575e-06 31250.0 100 0.246979591837 0.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 978.725775\n",
      "Stopping on iteration 28 since loss stagnates (loss = 2.195500, delta = 0.005518)\n",
      "2.575e-06 31250.0 200 0.290204081633 0.29\n",
      "iteration 0 / 1000: loss 960.461033\n",
      "Stopping on iteration 29 since loss stagnates (loss = 2.197913, delta = 0.001466)\n",
      "2.575e-06 31250.0 500 0.323653061224 0.327\n",
      "iteration 0 / 1000: loss 969.335701\n",
      "Stopping on iteration 28 since loss stagnates (loss = 2.164613, delta = 0.001873)\n",
      "2.575e-06 31250.0 1000 0.312897959184 0.324\n",
      "iteration 0 / 1000: loss 962.169479\n",
      "Stopping on iteration 28 since loss stagnates (loss = 2.170879, delta = 0.009261)\n",
      "2.575e-06 31250.0 1500 0.330408163265 0.34\n",
      "iteration 0 / 1000: loss 1144.098037\n",
      "Stopping on iteration 22 since loss stagnates (loss = 2.327050, delta = 0.002032)\n",
      "2.575e-06 37500.0 100 0.261081632653 0.26\n",
      "iteration 0 / 1000: loss 1143.454389\n",
      "Stopping on iteration 28 since loss stagnates (loss = 2.151511, delta = 0.001201)\n",
      "2.575e-06 37500.0 200 0.263836734694 0.275\n",
      "iteration 0 / 1000: loss 1163.257594\n",
      "Stopping on iteration 24 since loss stagnates (loss = 2.172752, delta = 0.003175)\n",
      "2.575e-06 37500.0 500 0.29687755102 0.306\n",
      "iteration 0 / 1000: loss 1154.771117\n",
      "Stopping on iteration 25 since loss stagnates (loss = 2.146606, delta = 0.009645)\n",
      "2.575e-06 37500.0 1000 0.300632653061 0.31\n",
      "iteration 0 / 1000: loss 1150.918437\n",
      "Stopping on iteration 26 since loss stagnates (loss = 2.140554, delta = 0.000244)\n",
      "2.575e-06 37500.0 1500 0.319816326531 0.32\n",
      "iteration 0 / 1000: loss 1349.773487\n",
      "Stopping on iteration 38 since loss stagnates (loss = 2.238431, delta = 0.009479)\n",
      "2.575e-06 43750.0 100 0.255081632653 0.265\n",
      "iteration 0 / 1000: loss 1366.877423\n",
      "Stopping on iteration 22 since loss stagnates (loss = 2.216809, delta = 0.007387)\n",
      "2.575e-06 43750.0 200 0.237591836735 0.241\n",
      "iteration 0 / 1000: loss 1323.134021\n",
      "Stopping on iteration 24 since loss stagnates (loss = 2.171155, delta = 0.004969)\n",
      "2.575e-06 43750.0 500 0.298816326531 0.301\n",
      "iteration 0 / 1000: loss 1352.573274\n",
      "Stopping on iteration 21 since loss stagnates (loss = 2.195832, delta = 0.001720)\n",
      "2.575e-06 43750.0 1000 0.294183673469 0.297\n",
      "iteration 0 / 1000: loss 1330.078960\n",
      "Stopping on iteration 23 since loss stagnates (loss = 2.151961, delta = 0.006236)\n",
      "2.575e-06 43750.0 1500 0.309734693878 0.313\n",
      "iteration 0 / 1000: loss 1523.514878\n",
      "Stopping on iteration 37 since loss stagnates (loss = 2.279914, delta = 0.008011)\n",
      "2.575e-06 50000.0 100 0.211306122449 0.233\n",
      "iteration 0 / 1000: loss 1539.801850\n",
      "Stopping on iteration 23 since loss stagnates (loss = 2.171291, delta = 0.000483)\n",
      "2.575e-06 50000.0 200 0.285448979592 0.284\n",
      "iteration 0 / 1000: loss 1540.849378\n",
      "Stopping on iteration 29 since loss stagnates (loss = 2.146546, delta = 0.003361)\n",
      "2.575e-06 50000.0 500 0.280612244898 0.299\n",
      "iteration 0 / 1000: loss 1545.365934\n",
      "Stopping on iteration 19 since loss stagnates (loss = 2.175476, delta = 0.003303)\n",
      "2.575e-06 50000.0 1000 0.303 0.305\n",
      "iteration 0 / 1000: loss 1519.169772\n",
      "Stopping on iteration 19 since loss stagnates (loss = 2.181746, delta = 0.002081)\n",
      "2.575e-06 50000.0 1500 0.299 0.318\n",
      "iteration 0 / 1000: loss 790.993096\n",
      "Stopping on iteration 14 since loss stagnates (loss = 2.843746, delta = 0.002022)\n",
      "5.05e-06 25000.0 100 0.204183673469 0.198\n",
      "iteration 0 / 1000: loss 784.594780\n",
      "Stopping on iteration 13 since loss stagnates (loss = 3.051538, delta = 0.005775)\n",
      "5.05e-06 25000.0 200 0.206734693878 0.213\n",
      "iteration 0 / 1000: loss 770.796699\n",
      "Stopping on iteration 35 since loss stagnates (loss = 2.462894, delta = 0.008820)\n",
      "5.05e-06 25000.0 500 0.189959183673 0.207\n",
      "iteration 0 / 1000: loss 769.036039\n",
      "Stopping on iteration 45 since loss stagnates (loss = 2.576989, delta = 0.007239)\n",
      "5.05e-06 25000.0 1000 0.184836734694 0.189\n",
      "iteration 0 / 1000: loss 784.742992\n",
      "Stopping on iteration 39 since loss stagnates (loss = 2.419992, delta = 0.003899)\n",
      "5.05e-06 25000.0 1500 0.199326530612 0.22\n",
      "iteration 0 / 1000: loss 982.890328\n",
      "Stopping on iteration 11 since loss stagnates (loss = 2.723288, delta = 0.000179)\n",
      "5.05e-06 31250.0 100 0.216653061224 0.217\n",
      "iteration 0 / 1000: loss 968.425583\n",
      "Stopping on iteration 27 since loss stagnates (loss = 2.581109, delta = 0.007835)\n",
      "5.05e-06 31250.0 200 0.216632653061 0.213\n",
      "iteration 0 / 1000: loss 976.122059\n",
      "Stopping on iteration 32 since loss stagnates (loss = 2.697159, delta = 0.000727)\n",
      "5.05e-06 31250.0 500 0.182734693878 0.197\n",
      "iteration 0 / 1000: loss 970.790219\n",
      "Stopping on iteration 58 since loss stagnates (loss = 2.836388, delta = 0.004429)\n",
      "5.05e-06 31250.0 1000 0.173 0.164\n",
      "iteration 0 / 1000: loss 973.979515\n",
      "Stopping on iteration 20 since loss stagnates (loss = 2.787256, delta = 0.005627)\n",
      "5.05e-06 31250.0 1500 0.168244897959 0.164\n",
      "iteration 0 / 1000: loss 1152.249501\n",
      "Stopping on iteration 16 since loss stagnates (loss = 3.481445, delta = 0.002348)\n",
      "5.05e-06 37500.0 100 0.182653061224 0.221\n",
      "iteration 0 / 1000: loss 1141.298322\n",
      "iteration 100 / 1000: loss 3.763910\n",
      "Stopping on iteration 161 since loss stagnates (loss = 2.615656, delta = 0.006849)\n",
      "5.05e-06 37500.0 200 0.199816326531 0.205\n",
      "iteration 0 / 1000: loss 1170.794215\n",
      "Stopping on iteration 11 since loss stagnates (loss = 2.579134, delta = 0.004902)\n",
      "5.05e-06 37500.0 500 0.178897959184 0.172\n",
      "iteration 0 / 1000: loss 1175.646259\n",
      "Stopping on iteration 76 since loss stagnates (loss = 2.965773, delta = 0.007459)\n",
      "5.05e-06 37500.0 1000 0.130142857143 0.124\n",
      "iteration 0 / 1000: loss 1167.656700\n",
      "Stopping on iteration 60 since loss stagnates (loss = 3.031603, delta = 0.002321)\n",
      "5.05e-06 37500.0 1500 0.145693877551 0.154\n",
      "iteration 0 / 1000: loss 1345.993933\n",
      "iteration 100 / 1000: loss 3.064327\n",
      "Stopping on iteration 106 since loss stagnates (loss = 2.775782, delta = 0.003983)\n",
      "5.05e-06 43750.0 100 0.170836734694 0.17\n",
      "iteration 0 / 1000: loss 1378.396746\n",
      "iteration 100 / 1000: loss 3.363476\n",
      "Stopping on iteration 101 since loss stagnates (loss = 3.363241, delta = 0.000235)\n",
      "5.05e-06 43750.0 200 0.187367346939 0.19\n",
      "iteration 0 / 1000: loss 1348.807374\n",
      "Stopping on iteration 30 since loss stagnates (loss = 3.516762, delta = 0.008692)\n",
      "5.05e-06 43750.0 500 0.179387755102 0.172\n",
      "iteration 0 / 1000: loss 1355.114235\n",
      "Stopping on iteration 25 since loss stagnates (loss = 3.372792, delta = 0.000080)\n",
      "5.05e-06 43750.0 1000 0.132346938776 0.132\n",
      "iteration 0 / 1000: loss 1366.393084\n",
      "Stopping on iteration 17 since loss stagnates (loss = 3.647827, delta = 0.007741)\n",
      "5.05e-06 43750.0 1500 0.132632653061 0.138\n",
      "iteration 0 / 1000: loss 1542.670703\n",
      "Stopping on iteration 37 since loss stagnates (loss = 2.644755, delta = 0.009665)\n",
      "5.05e-06 50000.0 100 0.179591836735 0.195\n",
      "iteration 0 / 1000: loss 1556.237171\n",
      "Stopping on iteration 34 since loss stagnates (loss = 3.429580, delta = 0.008104)\n",
      "5.05e-06 50000.0 200 0.0955306122449 0.103\n",
      "iteration 0 / 1000: loss 1536.816025\n",
      "Stopping on iteration 28 since loss stagnates (loss = 3.023360, delta = 0.002619)\n",
      "5.05e-06 50000.0 500 0.13787755102 0.125\n",
      "iteration 0 / 1000: loss 1546.392796\n",
      "Stopping on iteration 28 since loss stagnates (loss = 3.702609, delta = 0.003872)\n",
      "5.05e-06 50000.0 1000 0.177571428571 0.174\n",
      "iteration 0 / 1000: loss 1532.617977\n",
      "Stopping on iteration 96 since loss stagnates (loss = 3.953233, delta = 0.006258)\n",
      "5.05e-06 50000.0 1500 0.154387755102 0.143\n",
      "iteration 0 / 1000: loss 774.809812\n",
      "iteration 100 / 1000: loss 4.881404\n",
      "iteration 200 / 1000: loss 4.998184\n",
      "iteration 300 / 1000: loss 4.648082\n",
      "iteration 400 / 1000: loss 3.644735\n",
      "Stopping on iteration 466 since loss stagnates (loss = 5.145122, delta = 0.006599)\n",
      "7.525e-06 25000.0 100 0.149326530612 0.138\n",
      "iteration 0 / 1000: loss 780.858517\n",
      "Stopping on iteration 59 since loss stagnates (loss = 5.044250, delta = 0.008658)\n",
      "7.525e-06 25000.0 200 0.149408163265 0.169\n",
      "iteration 0 / 1000: loss 780.018978\n",
      "Stopping on iteration 28 since loss stagnates (loss = 5.512570, delta = 0.007607)\n",
      "7.525e-06 25000.0 500 0.140979591837 0.14\n",
      "iteration 0 / 1000: loss 782.448831\n",
      "Stopping on iteration 24 since loss stagnates (loss = 5.019493, delta = 0.004252)\n",
      "7.525e-06 25000.0 1000 0.153775510204 0.145\n",
      "iteration 0 / 1000: loss 765.051776\n",
      "iteration 100 / 1000: loss 4.212170\n",
      "iteration 200 / 1000: loss 4.562407\n",
      "Stopping on iteration 201 since loss stagnates (loss = 4.564462, delta = 0.002054)\n",
      "7.525e-06 25000.0 1500 0.113285714286 0.109\n",
      "iteration 0 / 1000: loss 970.101731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1000: loss 4.612678\n",
      "Stopping on iteration 133 since loss stagnates (loss = 6.952146, delta = 0.006433)\n",
      "7.525e-06 31250.0 100 0.144448979592 0.138\n",
      "iteration 0 / 1000: loss 956.238298\n",
      "Stopping on iteration 92 since loss stagnates (loss = 5.728586, delta = 0.004227)\n",
      "7.525e-06 31250.0 200 0.165612244898 0.153\n",
      "iteration 0 / 1000: loss 977.421790\n",
      "iteration 100 / 1000: loss 3.898101\n",
      "iteration 200 / 1000: loss 5.161252\n",
      "iteration 300 / 1000: loss 5.859011\n",
      "iteration 400 / 1000: loss 5.970360\n",
      "iteration 500 / 1000: loss 4.988693\n",
      "iteration 600 / 1000: loss 5.060298\n",
      "iteration 700 / 1000: loss 5.125371\n",
      "Stopping on iteration 778 since loss stagnates (loss = 5.357944, delta = 0.008426)\n",
      "7.525e-06 31250.0 500 0.166367346939 0.159\n",
      "iteration 0 / 1000: loss 977.716954\n",
      "iteration 100 / 1000: loss 5.265188\n",
      "iteration 200 / 1000: loss 4.750840\n",
      "iteration 300 / 1000: loss 4.767595\n",
      "iteration 400 / 1000: loss 5.144329\n",
      "iteration 500 / 1000: loss 5.088867\n",
      "iteration 600 / 1000: loss 4.217294\n",
      "iteration 700 / 1000: loss 5.100860\n",
      "iteration 800 / 1000: loss 4.835860\n",
      "iteration 900 / 1000: loss 4.484144\n",
      "7.525e-06 31250.0 1000 0.109224489796 0.106\n",
      "iteration 0 / 1000: loss 960.547808\n",
      "iteration 100 / 1000: loss 6.079142\n",
      "iteration 200 / 1000: loss 5.897624\n",
      "iteration 300 / 1000: loss 6.336035\n",
      "iteration 400 / 1000: loss 5.872697\n",
      "iteration 500 / 1000: loss 6.249627\n",
      "iteration 600 / 1000: loss 6.119108\n",
      "iteration 700 / 1000: loss 6.355008\n",
      "iteration 800 / 1000: loss 6.369701\n",
      "iteration 900 / 1000: loss 6.054799\n",
      "7.525e-06 31250.0 1500 0.168612244898 0.151\n",
      "iteration 0 / 1000: loss 1160.045498\n",
      "Stopping on iteration 51 since loss stagnates (loss = 6.528409, delta = 0.002367)\n",
      "7.525e-06 37500.0 100 0.12987755102 0.126\n",
      "iteration 0 / 1000: loss 1156.912791\n",
      "iteration 100 / 1000: loss 5.881758\n",
      "iteration 200 / 1000: loss 6.712054\n",
      "Stopping on iteration 234 since loss stagnates (loss = 6.999699, delta = 0.005514)\n",
      "7.525e-06 37500.0 200 0.116632653061 0.115\n",
      "iteration 0 / 1000: loss 1159.971635\n",
      "iteration 100 / 1000: loss 6.034299\n",
      "Stopping on iteration 125 since loss stagnates (loss = 6.515038, delta = 0.007981)\n",
      "7.525e-06 37500.0 500 0.0988979591837 0.111\n",
      "iteration 0 / 1000: loss 1167.622381\n",
      "iteration 100 / 1000: loss 6.014489\n",
      "iteration 200 / 1000: loss 5.676491\n",
      "iteration 300 / 1000: loss 5.778321\n",
      "iteration 400 / 1000: loss 5.362639\n",
      "iteration 500 / 1000: loss 5.426610\n",
      "iteration 600 / 1000: loss 5.746873\n",
      "iteration 700 / 1000: loss 5.351483\n",
      "iteration 800 / 1000: loss 5.451282\n",
      "iteration 900 / 1000: loss 5.746479\n",
      "7.525e-06 37500.0 1000 0.0952653061224 0.095\n",
      "iteration 0 / 1000: loss 1147.014629\n",
      "iteration 100 / 1000: loss 5.922483\n",
      "iteration 200 / 1000: loss 5.332055\n",
      "iteration 300 / 1000: loss 5.835347\n",
      "iteration 400 / 1000: loss 5.833373\n",
      "iteration 500 / 1000: loss 5.469568\n",
      "iteration 600 / 1000: loss 5.854950\n",
      "iteration 700 / 1000: loss 5.871670\n",
      "iteration 800 / 1000: loss 5.561864\n",
      "iteration 900 / 1000: loss 5.308541\n",
      "7.525e-06 37500.0 1500 0.097612244898 0.1\n",
      "iteration 0 / 1000: loss 1339.322177\n",
      "iteration 100 / 1000: loss 7.857972\n",
      "iteration 200 / 1000: loss 5.623716\n",
      "iteration 300 / 1000: loss 7.406775\n",
      "iteration 400 / 1000: loss 8.280113\n",
      "iteration 500 / 1000: loss 8.887571\n",
      "iteration 600 / 1000: loss 7.979454\n",
      "iteration 700 / 1000: loss 10.182384\n",
      "Stopping on iteration 758 since loss stagnates (loss = 9.206024, delta = 0.001948)\n",
      "7.525e-06 43750.0 100 0.133102040816 0.126\n",
      "iteration 0 / 1000: loss 1352.261284\n",
      "iteration 100 / 1000: loss 5.423167\n",
      "Stopping on iteration 104 since loss stagnates (loss = 7.043497, delta = 0.005048)\n",
      "7.525e-06 43750.0 200 0.15306122449 0.151\n",
      "iteration 0 / 1000: loss 1346.907530\n",
      "iteration 100 / 1000: loss 6.705466\n",
      "iteration 200 / 1000: loss 6.906611\n",
      "iteration 300 / 1000: loss 7.224470\n",
      "iteration 400 / 1000: loss 6.167764\n",
      "iteration 500 / 1000: loss 6.028631\n",
      "iteration 600 / 1000: loss 6.550824\n",
      "Stopping on iteration 620 since loss stagnates (loss = 6.971883, delta = 0.000779)\n",
      "7.525e-06 43750.0 500 0.159469387755 0.172\n",
      "iteration 0 / 1000: loss 1333.919990\n",
      "iteration 100 / 1000: loss 6.686056\n",
      "iteration 200 / 1000: loss 6.565487\n",
      "iteration 300 / 1000: loss 6.451205\n",
      "iteration 400 / 1000: loss 6.678879\n",
      "iteration 500 / 1000: loss 6.067910\n",
      "iteration 600 / 1000: loss 6.054516\n",
      "iteration 700 / 1000: loss 6.158180\n",
      "iteration 800 / 1000: loss 6.364575\n",
      "iteration 900 / 1000: loss 5.959165\n",
      "7.525e-06 43750.0 1000 0.0900612244898 0.087\n",
      "iteration 0 / 1000: loss 1340.431925\n",
      "iteration 100 / 1000: loss 6.396929\n",
      "iteration 200 / 1000: loss 6.052222\n",
      "iteration 300 / 1000: loss 6.909097\n",
      "iteration 400 / 1000: loss 7.027849\n",
      "iteration 500 / 1000: loss 6.801154\n",
      "iteration 600 / 1000: loss 6.605704\n",
      "iteration 700 / 1000: loss 6.538660\n",
      "iteration 800 / 1000: loss 6.212419\n",
      "iteration 900 / 1000: loss 6.198467\n",
      "Stopping on iteration 940 since loss stagnates (loss = 6.922698, delta = 0.006540)\n",
      "7.525e-06 43750.0 1500 0.149367346939 0.144\n",
      "iteration 0 / 1000: loss 1555.536465\n",
      "Stopping on iteration 52 since loss stagnates (loss = 9.062090, delta = 0.007030)\n",
      "7.525e-06 50000.0 100 0.110734693878 0.106\n",
      "iteration 0 / 1000: loss 1540.652640\n",
      "Stopping on iteration 84 since loss stagnates (loss = 9.146661, delta = 0.003015)\n",
      "7.525e-06 50000.0 200 0.0892857142857 0.094\n",
      "iteration 0 / 1000: loss 1515.071073\n",
      "Stopping on iteration 7 since loss stagnates (loss = 8.845413, delta = 0.007636)\n",
      "7.525e-06 50000.0 500 0.109204081633 0.092\n",
      "iteration 0 / 1000: loss 1557.819481\n",
      "iteration 100 / 1000: loss 7.554896\n",
      "iteration 200 / 1000: loss 7.374360\n",
      "iteration 300 / 1000: loss 7.725780\n",
      "iteration 400 / 1000: loss 7.146370\n",
      "iteration 500 / 1000: loss 7.567491\n",
      "iteration 600 / 1000: loss 7.436376\n",
      "iteration 700 / 1000: loss 8.035025\n",
      "iteration 800 / 1000: loss 7.735007\n",
      "iteration 900 / 1000: loss 7.115699\n",
      "7.525e-06 50000.0 1000 0.0787551020408 0.057\n",
      "iteration 0 / 1000: loss 1551.591555\n",
      "iteration 100 / 1000: loss 7.965576\n",
      "iteration 200 / 1000: loss 7.907341\n",
      "iteration 300 / 1000: loss 8.102547\n",
      "iteration 400 / 1000: loss 7.701474\n",
      "iteration 500 / 1000: loss 7.952212\n",
      "iteration 600 / 1000: loss 7.683266\n",
      "iteration 700 / 1000: loss 7.955775\n",
      "iteration 800 / 1000: loss 7.354735\n",
      "iteration 900 / 1000: loss 8.472808\n",
      "7.525e-06 50000.0 1500 0.0864489795918 0.089\n",
      "iteration 0 / 1000: loss 785.045737\n",
      "iteration 100 / 1000: loss 10.017224\n",
      "Stopping on iteration 171 since loss stagnates (loss = 6.275938, delta = 0.004841)\n",
      "1e-05 25000.0 100 0.134367346939 0.134\n",
      "iteration 0 / 1000: loss 771.696838\n",
      "iteration 100 / 1000: loss 6.375435\n",
      "iteration 200 / 1000: loss 8.620784\n",
      "Stopping on iteration 221 since loss stagnates (loss = 7.043875, delta = 0.008133)\n",
      "1e-05 25000.0 200 0.145428571429 0.151\n",
      "iteration 0 / 1000: loss 774.501188\n",
      "iteration 100 / 1000: loss 6.211560\n",
      "iteration 200 / 1000: loss 6.953481\n",
      "iteration 300 / 1000: loss 5.418149\n",
      "iteration 400 / 1000: loss 6.356917\n",
      "Stopping on iteration 492 since loss stagnates (loss = 7.993512, delta = 0.003481)\n",
      "1e-05 25000.0 500 0.122530612245 0.124\n",
      "iteration 0 / 1000: loss 776.493792\n",
      "iteration 100 / 1000: loss 6.709431\n",
      "iteration 200 / 1000: loss 7.279717\n",
      "iteration 300 / 1000: loss 6.935287\n",
      "iteration 400 / 1000: loss 7.694456\n",
      "iteration 500 / 1000: loss 7.087585\n",
      "iteration 600 / 1000: loss 8.047230\n",
      "Stopping on iteration 600 since loss stagnates (loss = 8.047230, delta = 0.001875)\n",
      "1e-05 25000.0 1000 0.150734693878 0.164\n",
      "iteration 0 / 1000: loss 767.858064\n",
      "iteration 100 / 1000: loss 7.376828\n",
      "Stopping on iteration 112 since loss stagnates (loss = 8.007557, delta = 0.007960)\n",
      "1e-05 25000.0 1500 0.123673469388 0.118\n",
      "iteration 0 / 1000: loss 976.907031\n",
      "Stopping on iteration 15 since loss stagnates (loss = 10.610091, delta = 0.008220)\n",
      "1e-05 31250.0 100 0.110489795918 0.099\n",
      "iteration 0 / 1000: loss 970.199126\n",
      "iteration 100 / 1000: loss 9.590184\n",
      "iteration 200 / 1000: loss 7.317153\n",
      "Stopping on iteration 214 since loss stagnates (loss = 10.256334, delta = 0.003016)\n",
      "1e-05 31250.0 200 0.151897959184 0.15\n",
      "iteration 0 / 1000: loss 964.750463\n",
      "Stopping on iteration 41 since loss stagnates (loss = 9.954353, delta = 0.007508)\n",
      "1e-05 31250.0 500 0.135469387755 0.144\n",
      "iteration 0 / 1000: loss 948.876501\n",
      "Stopping on iteration 96 since loss stagnates (loss = 9.276803, delta = 0.009427)\n",
      "1e-05 31250.0 1000 0.115163265306 0.108\n",
      "iteration 0 / 1000: loss 973.737204\n",
      "iteration 100 / 1000: loss 8.098968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1000: loss 8.137384\n",
      "iteration 300 / 1000: loss 7.848259\n",
      "iteration 400 / 1000: loss 8.328656\n",
      "iteration 500 / 1000: loss 8.345839\n",
      "iteration 600 / 1000: loss 8.629742\n",
      "iteration 700 / 1000: loss 7.778300\n",
      "iteration 800 / 1000: loss 7.976142\n",
      "iteration 900 / 1000: loss 8.522853\n",
      "1e-05 31250.0 1500 0.0972653061224 0.072\n",
      "iteration 0 / 1000: loss 1161.039763\n",
      "iteration 100 / 1000: loss 8.852046\n",
      "iteration 200 / 1000: loss 11.277033\n",
      "iteration 300 / 1000: loss 9.000461\n",
      "iteration 400 / 1000: loss 11.781984\n",
      "iteration 500 / 1000: loss 10.616447\n",
      "iteration 600 / 1000: loss 12.852773\n",
      "iteration 700 / 1000: loss 11.641444\n",
      "iteration 800 / 1000: loss 13.215834\n",
      "iteration 900 / 1000: loss 13.140224\n",
      "1e-05 37500.0 100 0.165387755102 0.162\n",
      "iteration 0 / 1000: loss 1159.945939\n",
      "iteration 100 / 1000: loss 11.271011\n",
      "iteration 200 / 1000: loss 11.161952\n",
      "iteration 300 / 1000: loss 10.167628\n",
      "iteration 400 / 1000: loss 12.066852\n",
      "iteration 500 / 1000: loss 9.019033\n",
      "iteration 600 / 1000: loss 11.798041\n",
      "Stopping on iteration 677 since loss stagnates (loss = 11.862108, delta = 0.003590)\n",
      "1e-05 37500.0 200 0.110285714286 0.11\n",
      "iteration 0 / 1000: loss 1156.193350\n",
      "iteration 100 / 1000: loss 11.057949\n",
      "Stopping on iteration 101 since loss stagnates (loss = 11.063780, delta = 0.005830)\n",
      "1e-05 37500.0 500 0.128020408163 0.132\n",
      "iteration 0 / 1000: loss 1161.001826\n",
      "iteration 100 / 1000: loss 10.904675\n",
      "iteration 200 / 1000: loss 10.141881\n",
      "iteration 300 / 1000: loss 10.948976\n",
      "iteration 400 / 1000: loss 10.578617\n",
      "iteration 500 / 1000: loss 9.696140\n",
      "iteration 600 / 1000: loss 10.175683\n",
      "iteration 700 / 1000: loss 10.355579\n",
      "iteration 800 / 1000: loss 10.489362\n",
      "iteration 900 / 1000: loss 10.277773\n",
      "1e-05 37500.0 1000 0.086693877551 0.093\n",
      "iteration 0 / 1000: loss 1144.137230\n",
      "Stopping on iteration 15 since loss stagnates (loss = 11.955309, delta = 0.006903)\n",
      "1e-05 37500.0 1500 0.108326530612 0.108\n",
      "iteration 0 / 1000: loss 1351.191818\n",
      "Stopping on iteration 29 since loss stagnates (loss = 11.940590, delta = 0.001415)\n",
      "1e-05 43750.0 100 0.0803265306122 0.078\n",
      "iteration 0 / 1000: loss 1357.948975\n",
      "iteration 100 / 1000: loss 13.408023\n",
      "Stopping on iteration 165 since loss stagnates (loss = 13.065303, delta = 0.006715)\n",
      "1e-05 43750.0 200 0.103693877551 0.116\n",
      "iteration 0 / 1000: loss 1348.434058\n",
      "iteration 100 / 1000: loss 11.777663\n",
      "iteration 200 / 1000: loss 13.944586\n",
      "iteration 300 / 1000: loss 12.060584\n",
      "iteration 400 / 1000: loss 12.218143\n",
      "iteration 500 / 1000: loss 11.413140\n",
      "iteration 600 / 1000: loss 13.982684\n",
      "iteration 700 / 1000: loss 12.933717\n",
      "iteration 800 / 1000: loss 11.573669\n",
      "iteration 900 / 1000: loss 12.448839\n",
      "1e-05 43750.0 500 0.080306122449 0.074\n",
      "iteration 0 / 1000: loss 1367.977811\n",
      "Stopping on iteration 64 since loss stagnates (loss = 13.578541, delta = 0.000692)\n",
      "1e-05 43750.0 1000 0.136367346939 0.138\n",
      "iteration 0 / 1000: loss 1347.984402\n",
      "iteration 100 / 1000: loss 13.142669\n",
      "iteration 200 / 1000: loss 12.792660\n",
      "Stopping on iteration 227 since loss stagnates (loss = 12.652152, delta = 0.002280)\n",
      "1e-05 43750.0 1500 0.106653061224 0.086\n",
      "iteration 0 / 1000: loss 1540.018908\n",
      "Stopping on iteration 57 since loss stagnates (loss = 15.138074, delta = 0.002018)\n",
      "1e-05 50000.0 100 0.0752857142857 0.08\n",
      "iteration 0 / 1000: loss 1551.015587\n",
      "iteration 100 / 1000: loss 16.318897\n",
      "iteration 200 / 1000: loss 17.475272\n",
      "iteration 300 / 1000: loss 16.098586\n",
      "iteration 400 / 1000: loss 15.233732\n",
      "Stopping on iteration 435 since loss stagnates (loss = 14.979173, delta = 0.006892)\n",
      "1e-05 50000.0 200 0.0716530612245 0.066\n",
      "iteration 0 / 1000: loss 1541.322755\n",
      "Stopping on iteration 37 since loss stagnates (loss = 17.353204, delta = 0.009162)\n",
      "1e-05 50000.0 500 0.0812040816327 0.079\n",
      "iteration 0 / 1000: loss 1552.135415\n",
      "Stopping on iteration 50 since loss stagnates (loss = 17.112737, delta = 0.005869)\n",
      "1e-05 50000.0 1000 0.109183673469 0.102\n",
      "iteration 0 / 1000: loss 1551.301192\n",
      "iteration 100 / 1000: loss 17.264970\n",
      "Stopping on iteration 162 since loss stagnates (loss = 16.545520, delta = 0.009815)\n",
      "1e-05 50000.0 1500 0.107571428571 0.114\n",
      "lr 1.000000e-07 reg 2.500000e+04 batch 100 train accuracy: 0.291143 val accuracy: 0.311000\n",
      "lr 1.000000e-07 reg 2.500000e+04 batch 200 train accuracy: 0.304735 val accuracy: 0.308000\n",
      "lr 1.000000e-07 reg 2.500000e+04 batch 500 train accuracy: 0.298776 val accuracy: 0.327000\n",
      "lr 1.000000e-07 reg 2.500000e+04 batch 1000 train accuracy: 0.307367 val accuracy: 0.334000\n",
      "lr 1.000000e-07 reg 2.500000e+04 batch 1500 train accuracy: 0.312306 val accuracy: 0.318000\n",
      "lr 1.000000e-07 reg 3.125000e+04 batch 100 train accuracy: 0.285102 val accuracy: 0.288000\n",
      "lr 1.000000e-07 reg 3.125000e+04 batch 200 train accuracy: 0.297245 val accuracy: 0.294000\n",
      "lr 1.000000e-07 reg 3.125000e+04 batch 500 train accuracy: 0.308347 val accuracy: 0.316000\n",
      "lr 1.000000e-07 reg 3.125000e+04 batch 1000 train accuracy: 0.311755 val accuracy: 0.331000\n",
      "lr 1.000000e-07 reg 3.125000e+04 batch 1500 train accuracy: 0.315224 val accuracy: 0.322000\n",
      "lr 1.000000e-07 reg 3.750000e+04 batch 100 train accuracy: 0.282327 val accuracy: 0.296000\n",
      "lr 1.000000e-07 reg 3.750000e+04 batch 200 train accuracy: 0.295408 val accuracy: 0.321000\n",
      "lr 1.000000e-07 reg 3.750000e+04 batch 500 train accuracy: 0.299367 val accuracy: 0.310000\n",
      "lr 1.000000e-07 reg 3.750000e+04 batch 1000 train accuracy: 0.302347 val accuracy: 0.312000\n",
      "lr 1.000000e-07 reg 3.750000e+04 batch 1500 train accuracy: 0.308204 val accuracy: 0.319000\n",
      "lr 1.000000e-07 reg 4.375000e+04 batch 100 train accuracy: 0.289367 val accuracy: 0.296000\n",
      "lr 1.000000e-07 reg 4.375000e+04 batch 200 train accuracy: 0.301347 val accuracy: 0.316000\n",
      "lr 1.000000e-07 reg 4.375000e+04 batch 500 train accuracy: 0.301898 val accuracy: 0.318000\n",
      "lr 1.000000e-07 reg 4.375000e+04 batch 1000 train accuracy: 0.304429 val accuracy: 0.323000\n",
      "lr 1.000000e-07 reg 4.375000e+04 batch 1500 train accuracy: 0.301796 val accuracy: 0.315000\n",
      "lr 1.000000e-07 reg 5.000000e+04 batch 100 train accuracy: 0.295939 val accuracy: 0.305000\n",
      "lr 1.000000e-07 reg 5.000000e+04 batch 200 train accuracy: 0.295673 val accuracy: 0.301000\n",
      "lr 1.000000e-07 reg 5.000000e+04 batch 500 train accuracy: 0.301694 val accuracy: 0.322000\n",
      "lr 1.000000e-07 reg 5.000000e+04 batch 1000 train accuracy: 0.300837 val accuracy: 0.314000\n",
      "lr 1.000000e-07 reg 5.000000e+04 batch 1500 train accuracy: 0.304735 val accuracy: 0.324000\n",
      "lr 2.575000e-06 reg 2.500000e+04 batch 100 train accuracy: 0.240673 val accuracy: 0.243000\n",
      "lr 2.575000e-06 reg 2.500000e+04 batch 200 train accuracy: 0.293184 val accuracy: 0.292000\n",
      "lr 2.575000e-06 reg 2.500000e+04 batch 500 train accuracy: 0.325347 val accuracy: 0.331000\n",
      "lr 2.575000e-06 reg 2.500000e+04 batch 1000 train accuracy: 0.317061 val accuracy: 0.342000\n",
      "lr 2.575000e-06 reg 2.500000e+04 batch 1500 train accuracy: 0.324224 val accuracy: 0.337000\n",
      "lr 2.575000e-06 reg 3.125000e+04 batch 100 train accuracy: 0.246980 val accuracy: 0.270000\n",
      "lr 2.575000e-06 reg 3.125000e+04 batch 200 train accuracy: 0.290204 val accuracy: 0.290000\n",
      "lr 2.575000e-06 reg 3.125000e+04 batch 500 train accuracy: 0.323653 val accuracy: 0.327000\n",
      "lr 2.575000e-06 reg 3.125000e+04 batch 1000 train accuracy: 0.312898 val accuracy: 0.324000\n",
      "lr 2.575000e-06 reg 3.125000e+04 batch 1500 train accuracy: 0.330408 val accuracy: 0.340000\n",
      "lr 2.575000e-06 reg 3.750000e+04 batch 100 train accuracy: 0.261082 val accuracy: 0.260000\n",
      "lr 2.575000e-06 reg 3.750000e+04 batch 200 train accuracy: 0.263837 val accuracy: 0.275000\n",
      "lr 2.575000e-06 reg 3.750000e+04 batch 500 train accuracy: 0.296878 val accuracy: 0.306000\n",
      "lr 2.575000e-06 reg 3.750000e+04 batch 1000 train accuracy: 0.300633 val accuracy: 0.310000\n",
      "lr 2.575000e-06 reg 3.750000e+04 batch 1500 train accuracy: 0.319816 val accuracy: 0.320000\n",
      "lr 2.575000e-06 reg 4.375000e+04 batch 100 train accuracy: 0.255082 val accuracy: 0.265000\n",
      "lr 2.575000e-06 reg 4.375000e+04 batch 200 train accuracy: 0.237592 val accuracy: 0.241000\n",
      "lr 2.575000e-06 reg 4.375000e+04 batch 500 train accuracy: 0.298816 val accuracy: 0.301000\n",
      "lr 2.575000e-06 reg 4.375000e+04 batch 1000 train accuracy: 0.294184 val accuracy: 0.297000\n",
      "lr 2.575000e-06 reg 4.375000e+04 batch 1500 train accuracy: 0.309735 val accuracy: 0.313000\n",
      "lr 2.575000e-06 reg 5.000000e+04 batch 100 train accuracy: 0.211306 val accuracy: 0.233000\n",
      "lr 2.575000e-06 reg 5.000000e+04 batch 200 train accuracy: 0.285449 val accuracy: 0.284000\n",
      "lr 2.575000e-06 reg 5.000000e+04 batch 500 train accuracy: 0.280612 val accuracy: 0.299000\n",
      "lr 2.575000e-06 reg 5.000000e+04 batch 1000 train accuracy: 0.303000 val accuracy: 0.305000\n",
      "lr 2.575000e-06 reg 5.000000e+04 batch 1500 train accuracy: 0.299000 val accuracy: 0.318000\n",
      "lr 5.050000e-06 reg 2.500000e+04 batch 100 train accuracy: 0.204184 val accuracy: 0.198000\n",
      "lr 5.050000e-06 reg 2.500000e+04 batch 200 train accuracy: 0.206735 val accuracy: 0.213000\n",
      "lr 5.050000e-06 reg 2.500000e+04 batch 500 train accuracy: 0.189959 val accuracy: 0.207000\n",
      "lr 5.050000e-06 reg 2.500000e+04 batch 1000 train accuracy: 0.184837 val accuracy: 0.189000\n",
      "lr 5.050000e-06 reg 2.500000e+04 batch 1500 train accuracy: 0.199327 val accuracy: 0.220000\n",
      "lr 5.050000e-06 reg 3.125000e+04 batch 100 train accuracy: 0.216653 val accuracy: 0.217000\n",
      "lr 5.050000e-06 reg 3.125000e+04 batch 200 train accuracy: 0.216633 val accuracy: 0.213000\n",
      "lr 5.050000e-06 reg 3.125000e+04 batch 500 train accuracy: 0.182735 val accuracy: 0.197000\n",
      "lr 5.050000e-06 reg 3.125000e+04 batch 1000 train accuracy: 0.173000 val accuracy: 0.164000\n",
      "lr 5.050000e-06 reg 3.125000e+04 batch 1500 train accuracy: 0.168245 val accuracy: 0.164000\n",
      "lr 5.050000e-06 reg 3.750000e+04 batch 100 train accuracy: 0.182653 val accuracy: 0.221000\n",
      "lr 5.050000e-06 reg 3.750000e+04 batch 200 train accuracy: 0.199816 val accuracy: 0.205000\n",
      "lr 5.050000e-06 reg 3.750000e+04 batch 500 train accuracy: 0.178898 val accuracy: 0.172000\n",
      "lr 5.050000e-06 reg 3.750000e+04 batch 1000 train accuracy: 0.130143 val accuracy: 0.124000\n",
      "lr 5.050000e-06 reg 3.750000e+04 batch 1500 train accuracy: 0.145694 val accuracy: 0.154000\n",
      "lr 5.050000e-06 reg 4.375000e+04 batch 100 train accuracy: 0.170837 val accuracy: 0.170000\n",
      "lr 5.050000e-06 reg 4.375000e+04 batch 200 train accuracy: 0.187367 val accuracy: 0.190000\n",
      "lr 5.050000e-06 reg 4.375000e+04 batch 500 train accuracy: 0.179388 val accuracy: 0.172000\n",
      "lr 5.050000e-06 reg 4.375000e+04 batch 1000 train accuracy: 0.132347 val accuracy: 0.132000\n",
      "lr 5.050000e-06 reg 4.375000e+04 batch 1500 train accuracy: 0.132633 val accuracy: 0.138000\n",
      "lr 5.050000e-06 reg 5.000000e+04 batch 100 train accuracy: 0.179592 val accuracy: 0.195000\n",
      "lr 5.050000e-06 reg 5.000000e+04 batch 200 train accuracy: 0.095531 val accuracy: 0.103000\n",
      "lr 5.050000e-06 reg 5.000000e+04 batch 500 train accuracy: 0.137878 val accuracy: 0.125000\n",
      "lr 5.050000e-06 reg 5.000000e+04 batch 1000 train accuracy: 0.177571 val accuracy: 0.174000\n",
      "lr 5.050000e-06 reg 5.000000e+04 batch 1500 train accuracy: 0.154388 val accuracy: 0.143000\n",
      "lr 7.525000e-06 reg 2.500000e+04 batch 100 train accuracy: 0.149327 val accuracy: 0.138000\n",
      "lr 7.525000e-06 reg 2.500000e+04 batch 200 train accuracy: 0.149408 val accuracy: 0.169000\n",
      "lr 7.525000e-06 reg 2.500000e+04 batch 500 train accuracy: 0.140980 val accuracy: 0.140000\n",
      "lr 7.525000e-06 reg 2.500000e+04 batch 1000 train accuracy: 0.153776 val accuracy: 0.145000\n",
      "lr 7.525000e-06 reg 2.500000e+04 batch 1500 train accuracy: 0.113286 val accuracy: 0.109000\n",
      "lr 7.525000e-06 reg 3.125000e+04 batch 100 train accuracy: 0.144449 val accuracy: 0.138000\n",
      "lr 7.525000e-06 reg 3.125000e+04 batch 200 train accuracy: 0.165612 val accuracy: 0.153000\n",
      "lr 7.525000e-06 reg 3.125000e+04 batch 500 train accuracy: 0.166367 val accuracy: 0.159000\n",
      "lr 7.525000e-06 reg 3.125000e+04 batch 1000 train accuracy: 0.109224 val accuracy: 0.106000\n",
      "lr 7.525000e-06 reg 3.125000e+04 batch 1500 train accuracy: 0.168612 val accuracy: 0.151000\n",
      "lr 7.525000e-06 reg 3.750000e+04 batch 100 train accuracy: 0.129878 val accuracy: 0.126000\n",
      "lr 7.525000e-06 reg 3.750000e+04 batch 200 train accuracy: 0.116633 val accuracy: 0.115000\n",
      "lr 7.525000e-06 reg 3.750000e+04 batch 500 train accuracy: 0.098898 val accuracy: 0.111000\n",
      "lr 7.525000e-06 reg 3.750000e+04 batch 1000 train accuracy: 0.095265 val accuracy: 0.095000\n",
      "lr 7.525000e-06 reg 3.750000e+04 batch 1500 train accuracy: 0.097612 val accuracy: 0.100000\n",
      "lr 7.525000e-06 reg 4.375000e+04 batch 100 train accuracy: 0.133102 val accuracy: 0.126000\n",
      "lr 7.525000e-06 reg 4.375000e+04 batch 200 train accuracy: 0.153061 val accuracy: 0.151000\n",
      "lr 7.525000e-06 reg 4.375000e+04 batch 500 train accuracy: 0.159469 val accuracy: 0.172000\n",
      "lr 7.525000e-06 reg 4.375000e+04 batch 1000 train accuracy: 0.090061 val accuracy: 0.087000\n",
      "lr 7.525000e-06 reg 4.375000e+04 batch 1500 train accuracy: 0.149367 val accuracy: 0.144000\n",
      "lr 7.525000e-06 reg 5.000000e+04 batch 100 train accuracy: 0.110735 val accuracy: 0.106000\n",
      "lr 7.525000e-06 reg 5.000000e+04 batch 200 train accuracy: 0.089286 val accuracy: 0.094000\n",
      "lr 7.525000e-06 reg 5.000000e+04 batch 500 train accuracy: 0.109204 val accuracy: 0.092000\n",
      "lr 7.525000e-06 reg 5.000000e+04 batch 1000 train accuracy: 0.078755 val accuracy: 0.057000\n",
      "lr 7.525000e-06 reg 5.000000e+04 batch 1500 train accuracy: 0.086449 val accuracy: 0.089000\n",
      "lr 1.000000e-05 reg 2.500000e+04 batch 100 train accuracy: 0.134367 val accuracy: 0.134000\n",
      "lr 1.000000e-05 reg 2.500000e+04 batch 200 train accuracy: 0.145429 val accuracy: 0.151000\n",
      "lr 1.000000e-05 reg 2.500000e+04 batch 500 train accuracy: 0.122531 val accuracy: 0.124000\n",
      "lr 1.000000e-05 reg 2.500000e+04 batch 1000 train accuracy: 0.150735 val accuracy: 0.164000\n",
      "lr 1.000000e-05 reg 2.500000e+04 batch 1500 train accuracy: 0.123673 val accuracy: 0.118000\n",
      "lr 1.000000e-05 reg 3.125000e+04 batch 100 train accuracy: 0.110490 val accuracy: 0.099000\n",
      "lr 1.000000e-05 reg 3.125000e+04 batch 200 train accuracy: 0.151898 val accuracy: 0.150000\n",
      "lr 1.000000e-05 reg 3.125000e+04 batch 500 train accuracy: 0.135469 val accuracy: 0.144000\n",
      "lr 1.000000e-05 reg 3.125000e+04 batch 1000 train accuracy: 0.115163 val accuracy: 0.108000\n",
      "lr 1.000000e-05 reg 3.125000e+04 batch 1500 train accuracy: 0.097265 val accuracy: 0.072000\n",
      "lr 1.000000e-05 reg 3.750000e+04 batch 100 train accuracy: 0.165388 val accuracy: 0.162000\n",
      "lr 1.000000e-05 reg 3.750000e+04 batch 200 train accuracy: 0.110286 val accuracy: 0.110000\n",
      "lr 1.000000e-05 reg 3.750000e+04 batch 500 train accuracy: 0.128020 val accuracy: 0.132000\n",
      "lr 1.000000e-05 reg 3.750000e+04 batch 1000 train accuracy: 0.086694 val accuracy: 0.093000\n",
      "lr 1.000000e-05 reg 3.750000e+04 batch 1500 train accuracy: 0.108327 val accuracy: 0.108000\n",
      "lr 1.000000e-05 reg 4.375000e+04 batch 100 train accuracy: 0.080327 val accuracy: 0.078000\n",
      "lr 1.000000e-05 reg 4.375000e+04 batch 200 train accuracy: 0.103694 val accuracy: 0.116000\n",
      "lr 1.000000e-05 reg 4.375000e+04 batch 500 train accuracy: 0.080306 val accuracy: 0.074000\n",
      "lr 1.000000e-05 reg 4.375000e+04 batch 1000 train accuracy: 0.136367 val accuracy: 0.138000\n",
      "lr 1.000000e-05 reg 4.375000e+04 batch 1500 train accuracy: 0.106653 val accuracy: 0.086000\n",
      "lr 1.000000e-05 reg 5.000000e+04 batch 100 train accuracy: 0.075286 val accuracy: 0.080000\n",
      "lr 1.000000e-05 reg 5.000000e+04 batch 200 train accuracy: 0.071653 val accuracy: 0.066000\n",
      "lr 1.000000e-05 reg 5.000000e+04 batch 500 train accuracy: 0.081204 val accuracy: 0.079000\n",
      "lr 1.000000e-05 reg 5.000000e+04 batch 1000 train accuracy: 0.109184 val accuracy: 0.102000\n",
      "lr 1.000000e-05 reg 5.000000e+04 batch 1500 train accuracy: 0.107571 val accuracy: 0.114000\n",
      "best validation accuracy achieved during cross-validation: 0.342000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 1e-5]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "batch_sizes = (100, 200, 500, 1000, 1500)\n",
    "\n",
    "LEARNING_RATES = 5\n",
    "REG_STRENGTHS = 5\n",
    "ITERATIONS = 1000\n",
    "\n",
    "for learning_rate in np.linspace(*learning_rates, LEARNING_RATES):\n",
    "    for reg in np.linspace(*regularization_strengths, REG_STRENGTHS):\n",
    "        for batch_size in batch_sizes:\n",
    "            softmax = Softmax()\n",
    "            softmax.train(X_train, y_train, learning_rate, reg, ITERATIONS, batch_size, verbose=True)\n",
    "            \n",
    "            training_predictions = softmax.predict(X_train)\n",
    "            training_accuracy = np.mean(y_train == training_predictions)\n",
    "            \n",
    "            validation_predictions = softmax.predict(X_val)\n",
    "            validation_accuracy = np.mean(y_val == validation_predictions)\n",
    "            print(learning_rate, reg, batch_size, training_accuracy, validation_accuracy)\n",
    "            \n",
    "            results[learning_rate, reg, batch_size] = training_accuracy, validation_accuracy\n",
    "            if validation_accuracy > best_val:\n",
    "                print('Found best validation accuracy so far', validation_accuracy)\n",
    "                best_val = validation_accuracy\n",
    "                best_softmax = softmax\n",
    "                \n",
    "                \n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg, batch in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg, batch)]\n",
    "    print('lr %e reg %e batch %d train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, batch, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.327000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmYIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113b1d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xw = np.matmul(X_dev, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxes = xw[np.arange(500), np.argmax(xw, 1)]\n",
    "maxes.shape = (500, 1)\n",
    "fixed = xw - maxes\n",
    "np.sum(fixed > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xw, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3073, 10), (500, 10), (500, 3073))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, xw.shape, X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
